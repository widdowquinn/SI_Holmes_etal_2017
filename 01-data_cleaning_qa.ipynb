{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Information: Holmes *et al.* 2017\n",
    "\n",
    "# 1. Data cleaning, normalisation and quality assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "Raw data was previously converted from two `Excel` files:\n",
    "\n",
    "* `AH alldata 12082013.xlsx` was converted to `data/treatment_unix_endings.csv`\n",
    "* `AH alldata expt1 flagged 05092013.xlsx` was converted to `data/control_unix_endings_flags.csv`\n",
    "\n",
    "These describe microarray results for samples that underwent two treatments:\n",
    "\n",
    "* *in vitro* growth only - **control** - `data/control_unix_endings_flags.csv`\n",
    "* *in vitro* growth and plant passage - **treatment** - `data/treatment_unix_endings.csv`\n",
    "\n",
    "We import these data into corresponding `treatment` and `control` dataframes, with the array probe systematic name as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "control = pd.read_csv('data/control_unix_endings_flags.csv', sep=',', skiprows=4, index_col=0)\n",
    "treatment = pd.read_csv('data/treatment_unix_endings.csv', sep=',', skiprows=4, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the full dataset to only the raw intensity values, as `control_raw` and `treatment_raw`. In both runs, the mapping of experimental samples (*input* and *output*) across the three replicates is:\n",
    "\n",
    "* replicate 1 *input* $\\rightarrow$ `Raw`\n",
    "* replicate 1 *output* $\\rightarrow$ `Raw.1`\n",
    "* replicate 2 *input* $\\rightarrow$ `Raw.2`\n",
    "* replicate 2 *output* $\\rightarrow$ `Raw.3`\n",
    "* replicate 3 *input* $\\rightarrow$ `Raw.4`\n",
    "* replicate 3 *output* $\\rightarrow$ `Raw.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colnames_in = ['Raw', 'Raw.1', 'Raw.2', 'Raw.3', 'Raw.4', 'Raw.5']\n",
    "colnames_out = ['input.1', 'output.1', 'input.2', 'output.2', 'input.3', 'output.3']\n",
    "\n",
    "# reduce data to raw intensity values\n",
    "control_raw = control[colnames_in]\n",
    "control_raw.columns = colnames_out\n",
    "treatment_raw = treatment[colnames_in]\n",
    "treatment_raw.columns = colnames_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "We can inspect the extent of correlation between the datasets visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = scipy.stats.pearsonr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.3f}\".format(r),\n",
    "                xy=(.3, .5), size=15,\n",
    "                xycoords=ax.transAxes)\n",
    "    \n",
    "def plot_correlation(data, title=None):\n",
    "    g = sns.PairGrid(data)\n",
    "    g.map_lower(plt.scatter)\n",
    "    g.map_diag(sns.kdeplot, legend=False)\n",
    "    g.map_upper(corrfunc)\n",
    "    g.set(xticklabels=[])\n",
    "    g.set(title=title or '')\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot control dataset correlation\n",
    "plot_correlation(control_raw);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot control dataset correlation\n",
    "plot_correlation(treatment_raw);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots indicate:\n",
    "\n",
    "* the intensities of the control arrays are systematically larger than the treatment arrays, suggesting that the effects of noise may be proportionally greater for the treatment arrays\n",
    "* the control arrays are good candidates for quantile normalisation (QN; $r > 0.93$, with similar density distributions)\n",
    "* the treatment array `input.3` dataset is potentially problematic for , due to three datapoints with intensities greater than 40,000 units having large leverage.\n",
    "\n",
    "We can identify the troublesome probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select outlying treatment input.3 values\n",
    "treatment_raw.loc[treatment_raw['input.3'] > 4e4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the influence of these three probes, the treatment datasets show good correlation, and strong similarity in distribution, so we are justified in using quantile normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pearson correlation coefficients for treatment data, excluding outliers\n",
    "treatment_raw.loc[treatment_raw['input.3'] < 4e4].corr('pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spearman correlation coefficients for treatment data, including outliers\n",
    "treatment_raw.corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise control intensity distributions\n",
    "g = sns.violinplot(log(control_raw))\n",
    "g.set_title(\"Control log(intensity) distributions\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise treatment intensity distributions (with outliers)\n",
    "g = sns.violinplot(log(treatment_raw))\n",
    "g.set_title(\"Treatment log(intensity) distributions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these distributions it appears that treatment `output.3` appears to have a higher baseline signal than the other treatment arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile normalisation\n",
    "\n",
    "We have established that because the input and output arrays in both control and treatment conditions have strong correlation across all intensities and similar intensity distributions, we are justified in using quantile (mean) normalisation.\n",
    "\n",
    "As we expect the overall effect on array signal to vary according to whether the sample was from the input (strong) or output (weak) set, and whether the sample came from the control or treatment pools, we divide the dataset into four components, and apply normalisation to four arrays independently, as subsets of the raw datasets `control_raw` and `treatment_raw`:\n",
    "\n",
    "* `control_input`\n",
    "* `control_output`\n",
    "* `treatment_input`\n",
    "* `treatment_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quantile_norm(df, columns=None):\n",
    "    \"\"\"Normalise the columns of df to each have the same distribution\"\"\"\n",
    "    df_matrix = df.as_matrix(columns=columns)\n",
    "    quantiles = np.mean(np.sort(df_matrix, axis=0), axis=1)\n",
    "    ranks = scipy.stats.mstats.rankdata(df_matrix, axis=0).astype(int) - 1\n",
    "    norm_matrix = quantiles[ranks]\n",
    "    return(pd.DataFrame(data=norm_matrix, index=df.index,\n",
    "                        columns=columns or df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalise inputs and outputs for control and treatment separately\n",
    "control_input = quantile_norm(control_raw, columns=['input.1', 'input.2', 'input.3'])\n",
    "control_output = quantile_norm(control_raw, columns=['output.1', 'output.2', 'output.3'])\n",
    "treatment_input = quantile_norm(treatment_raw, columns=['input.1', 'input.2', 'input.3'])\n",
    "treatment_output = quantile_norm(treatment_raw, columns=['output.1', 'output.2', 'output.3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the effect this has on the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12,6))\n",
    "fig.subplots_adjust(hspace=.25)\n",
    "axes = axes.ravel()\n",
    "for ttl, arr, ax in zip((\"control input\", \"control output\", \"treatment input\", \"treatment output\"),\n",
    "                        (control_input, control_output, treatment_input, treatment_output),\n",
    "                        axes):\n",
    "    ax.set_title(ttl)\n",
    "    sns.violinplot(log(arr), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ttl, arr in zip((\"control input\", \"control output\", \"treatment input\", \"treatment output\"),\n",
    "                    (control_input, control_output, treatment_input, treatment_output)):\n",
    "    plot_correlation(arr, title=ttl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Collecting data\n",
    "\n",
    "We now have four dataframes, each indexed by array probe systematic name, with three columns that correspond to replicates 1, 2, and 3 for either a control or a treatment run. For downstream analysis we want to organise this data as the following columns:\n",
    "\n",
    "* `index`: unique ID\n",
    "* `probe`: probe name (these apply across treatment/control and input/output)\n",
    "* `input`: normalised input intensity value (for a particular probe and replicate)\n",
    "* `output`: normalised input intensity value (for a particular probe and replicate)\n",
    "* `treatment`: 0/1 indicating whether the measurement was made for the control or treatment sample\n",
    "* `replicate`: 1, 2, 3 indicating which replicate the measurement was made from\n",
    "\n",
    "We can have other columns too, but for downstream analysis we want the above columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide to long form\n",
    "\n",
    "First, we convert our data frames from wide (e.g. `input.1`, `input.2`, `input.3` columns) to long (e.g. `probe`, `input`, `output`, `replicate`) form - once for the control data, and once for the treatment data. We match on a multi-index of probe and replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wide_to_long(df, stage):\n",
    "    if not stage:\n",
    "        stagestr = 'input'\n",
    "    else:\n",
    "        stagestr = 'output'\n",
    "\n",
    "    df.reset_index(level=0, inplace=True)  # make probes a column\n",
    "    df = pd.melt(df, id_vars=['Systematic'],\n",
    "                 value_vars=['{0}.1'.format(stagestr),\n",
    "                             '{0}.2'.format(stagestr),\n",
    "                             '{0}.3'.format(stagestr)])\n",
    "    df.columns = ['probe', 'class', stagestr]\n",
    "    df.loc[:, 'replicate'] = df['class'].astype(str).str[-1].astype(np.int64)\n",
    "    df = df[['probe', 'replicate', stagestr]]\n",
    "    df.set_index(['probe', 'replicate'], inplace=True)\n",
    "    return df\n",
    "        \n",
    "\n",
    "def wide_to_long_join(df_in, df_out, treatment):\n",
    "    if treatment:\n",
    "        treatval = 1\n",
    "    else:\n",
    "        treatval = 0\n",
    "            \n",
    "    df = pd.merge(wide_to_long(df_in, 0), wide_to_long(df_out, 1),\n",
    "                  left_index=True, right_index=True)\n",
    "    df['treatment'] = treatval\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert control and treatment data to long format\n",
    "control_long = wide_to_long_join(control_input, control_output, treatment=False)\n",
    "treatment_long = wide_to_long_join(treatment_input, treatment_output, treatment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we combine them into a single, long-form dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "control_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine dataframes\n",
    "data = control_long.append(treatment_long, ignore_index=True)\n",
    "data['log_input'] = log(data['input'])\n",
    "data['log_output'] = log(data['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it easier to gather up groups of data by their labelled properties, as in the violin plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_v_output = pd.melt(data, id_vars=['probe', 'replicate', 'treatment'],\n",
    "                         value_vars=['log_input', 'log_output'])\n",
    "input_v_output.columns = ['probe', 'replicate', 'treatment',\n",
    "                          'stage', 'log_intensity']\n",
    "input_v_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.violinplot(data=input_v_output, x=\"treatment\", y=\"log_intensity\",\n",
    "                   hue=\"stage\", split=True)\n",
    "g.set_xticklabels(['control', 'treatment'])\n",
    "g.set_ylabel(\"log(intensity)\")\n",
    "g.set_xlabel(\"\")\n",
    "g.set_title(\"log(intensity) distribution by treatment and input/output\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sakai vs DH10B data\n",
    "\n",
    "We are interested only in the potential for differential representation of *E.coli* Sakai protein products, and could later remove all probes that are not expected to hybridise to *E. coli* Sakai from the dataset. We are not concerned with high-level interactions with *E. coli* DH10B genes, and may be willing later on to exclude these from the analysis.\n",
    "\n",
    "Evidence for potential hybridisation to Sakai was determined by `BLASTX` query of each probe sequence against chromosome and plasmid amino acid sequences from the NCBI records `NC_002695`, `NC_002128` and `NC_002127`. The results of the `BLASTX` comparison are provided in the file `data/probes_blastx_sakai.tab`.\n",
    "\n",
    "We read the data from the `data/probes_blastx_sakai.tab` file, and add a new column called `sakai`, which takes a value of `1` if the probe could hybridise to Sakai, and `0` if a `BLASTX` hit is not recorded, or a `BLASTX` hit with less than 100% identity is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read BLASTX hit data, and discard probes with less than 100% identity match\n",
    "blast_hits = pd.read_csv(\"data/probes_blastx_sakai.tab\", sep=\"\\t\",\n",
    "                         names=['probe', 'match', 'identity', 'length',\n",
    "                                'mismatch', 'gapopen', 'qstart', 'qend',\n",
    "                                'sstart', 'send', 'evalue', 'bitscore'])\n",
    "blast_hits = blast_hits.loc[blast_hits['identity'] == 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new column to our dataset, on the basis of the `probe` column from `data` being present in `blast_hits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add sakai column: 1 if probe has 100% BLASTX hit, 0 otherwise\n",
    "data['sakai'] = data['probe'].isin(blast_hits['probe'].values).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data\n",
    "\n",
    "We write the normalised, long-format data to `output/normalised_array_data.tab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write data to file\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "data.to_csv(\"output/normalised_array_data.tab\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Later we will also want to assign and index for common probe ID to each probe, and we can do that here, writing out the indexed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create indices and values for probes\n",
    "probe_ids = data['probe'].unique()\n",
    "nprobes = len(probe_ids)\n",
    "probe_lookup = dict(zip(probe_ids, range(nprobes)))\n",
    "\n",
    "# add data column with probe index from probe_lookup\n",
    "data['probe_index'] = data['probe'].replace(probe_lookup).values\n",
    "\n",
    "# write indexed data to file for later use\n",
    "data.to_csv(\"output/normalised_indexed_array_data.tab\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, we will work with a small subset of the data to develop the models/code without excessive compute time. For this we subset 200 probe IDs - but this means we need to reindex the probes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make reduced set of probes for testing\n",
    "random.seed(325612823)  # for reproducibility of random choice\n",
    "probe_indices = [random.randint(0, nprobes) for i in range(2000)]  # 2000 random probe ID indices\n",
    "reduced = data.loc[data['probe_index'].isin(probe_indices)]\n",
    "\n",
    "# create indices and values for probes\n",
    "probe_ids = reduced['probe'].unique()\n",
    "nprobes = len(probe_ids)\n",
    "probe_lookup = dict(zip(probe_ids, range(nprobes)))\n",
    "\n",
    "# add data column with probe index from probe_lookup\n",
    "reduced['probe_index'] = reduced['probe'].replace(probe_lookup).values\n",
    "\n",
    "# write test data\n",
    "reduced.to_csv(\"output/reduced_normalised_indexed_array_data.tab\",\n",
    "               sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
